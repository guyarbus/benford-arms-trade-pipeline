{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80264bd6",
   "metadata": {},
   "source": [
    "# NOTEBOOK 01 : DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cfa9f2",
   "metadata": {},
   "source": [
    "**Author** : Guy Arbus\n",
    "**Date** : 2026/02/09\n",
    "**Version** : 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e795283",
   "metadata": {},
   "source": [
    "## ABSTRACT\n",
    "\n",
    "The data cleaning phase aimed to ensure the **reliability**, **consistency**, and **operational relevance** of the dataset prior to analysis and modeling.  \n",
    "\n",
    "Raw data were systematically inspected to identify missing values, outliers, duplicates, and incoherent records, particularly those resulting from sensor noise, transmission errors, or adversarial perturbations. Domain-specific rules and statistical thresholds were applied to validate measurements and enforce physical plausibility constraints. Missing or corrupted data were handled using controlled imputation strategies or exclusion when necessary to avoid bias.  \n",
    "\n",
    "This process was essential to reduce noise, enhance signal integrity, and guarantee that downstream analytical and predictive models operate on data aligned with defense and security requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904e779",
   "metadata": {},
   "source": [
    "## BEST PRACTICES & PRINCIPLES\n",
    "\n",
    "- Always keep raw data unchanged (read-only)\n",
    "- Document every decision and assumption\n",
    "- Version control your cleaning scripts\n",
    "- Make it reproducible (seeds, scripts, environments)\n",
    "- Validate at each step before proceeding\n",
    "- Visualize before and after each major change\n",
    "- Domain expertise consultation when uncertain\n",
    "- Automate repetitive tasks\n",
    "- Test on sample before applying to full dataset\n",
    "- Consider ethical implications (bias, fairness, privacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cbef9d",
   "metadata": {},
   "source": [
    "## COMMON PITFALLS TO AVOID\n",
    "\n",
    "- Deleting data without understanding why\n",
    "- Imputing without investigating missingness patterns\n",
    "- Removing outliers that are legitimate\n",
    "- Data leakage (using test set info in cleaning)\n",
    "- Over-cleaning (removing valuable variance)\n",
    "- Ignoring domain knowledge\n",
    "- Not documenting changes\n",
    "- Cleaning test data differently than training data\n",
    "- Assuming correlation implies causation in feature selection\n",
    "- Not validating cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e709a3",
   "metadata": {},
   "source": [
    "## TABLE OF CONTENT\n",
    "\n",
    "### 01. INITIAL DATA ASSESSMENT\n",
    "\n",
    "#### Data Sources\n",
    "#### Collection methods\n",
    "#### Sensitive data classification\n",
    "#### Restrictions\n",
    "#### Data dictionary\n",
    "#### Completeness\n",
    "#### Data structure \n",
    "\n",
    "---\n",
    "\n",
    "### 02. HANDLING SENSITIVE DATA AND CLASSIFICATION INFORMATION\n",
    "\n",
    "---\n",
    "\n",
    "### 03. STRUCTURAL ISSUES\n",
    "\n",
    "---\n",
    "\n",
    "### 04. MISSING DATA\n",
    "\n",
    "#### 04.1 Missing data identification\n",
    "\n",
    "**Calculate missingness percentage per variable**  \n",
    "**Identify patterns (MCAR, MAR, MNAR)**  \n",
    "**Create missingness indicator variables**  \n",
    "**Visualize missingness patterns (heatmaps, co-occurrence matrices)**  \n",
    "**Document reasons for missingness when known**  \n",
    "\n",
    "#### 04.2 Missing data treatment\n",
    "\n",
    "**Determine appropriate imputation method per variable**  \n",
    "**Document deletion criteria (listwise, pairwise)**  \n",
    "**Validate imputation quality**  \n",
    "**Consider multiple imputation for critical variables**  \n",
    "**Flag imputed values for transparency**  \n",
    "\n",
    "---\n",
    "\n",
    "### 05. DUPLICATES\n",
    "\n",
    "#### 05.1 Exact Duplicates\n",
    "\n",
    "**Identify complete row duplicates**  \n",
    "**Check for duplicate IDs/primary keys**  \n",
    "**Verify timestamp duplicates in time-series data**  \n",
    "**Document retention policy (first, last, average)**  \n",
    "  \n",
    "####Â 05.2 Fuzzy Duplicates\n",
    "\n",
    "**Implement similarity matching for text fields**  \n",
    "**Check for near-duplicate sensor readings**  \n",
    "**Identify potential entity resolution issues**  \n",
    "**Apply domain-specific deduplication rules**  \n",
    "\n",
    "---\n",
    "\n",
    "### 06. OUTLIERS\n",
    "\n",
    "#### 06.1 Statistical Outliers\n",
    "\n",
    "**Apply univariate outlier detection (IQR, Z-score, modified Z-score)**  \n",
    "**Implement multivariate outlier detection (Mahalanobis distance, isolation forests)**  \n",
    "**Check for distribution violations**  \n",
    "**Validate against known operational ranges**  \n",
    "\n",
    "#### 06.2 Domain-Specific Anomalies\n",
    "\n",
    "**Verify physically impossible values (negative distances, speeds exceeding limits)**  \n",
    "**Check temporal consistency (events out of chronological order)**  \n",
    "**Validate geographic coordinates within operational theaters**  \n",
    "**Verify sensor readings against calibration ranges**  \n",
    "**Check for adversarial data contamination**  \n",
    "\n",
    "#### 06.3 Outlier Treatment\n",
    "\n",
    "**Distinguish errors from legitimate extreme values**  \n",
    "**Document retention/removal decisions**  \n",
    "**Consider winsorization or transformation**  \n",
    "**Flag outliers for analyst review**  \n",
    "\n",
    "---\n",
    "\n",
    "### 07. CONSISTENCY\n",
    "\n",
    "#### 07.1 Internal Consistency\n",
    "\n",
    "**Cross-validate related fields (e.g., start/end times)**  \n",
    "**Check calculated fields against source data**  \n",
    "**Verify hierarchical relationships (category/subcategory)**  \n",
    "**Validate conditional logic rules**  \n",
    "**Check for contradictory information**  \n",
    "\n",
    "#### 07.2 External Validation\n",
    "\n",
    "**Compare against authoritative reference data**  \n",
    "**Validate geographic locations against maps**  \n",
    "**Cross-reference with external databases**  \n",
    "**Verify equipment specifications against technical documentation**  \n",
    "**Check historical consistency with previous datasets**  \n",
    "\n",
    "#### 07.3 Business/Domain Rules\n",
    "\n",
    "**Apply defense-specific validation rules**  \n",
    "**Verify mission-critical constraints**  \n",
    "**Check operational parameter bounds**  \n",
    "**Validate threat classification schemas**  \n",
    "**Ensure compliance with doctrine and procedures**  \n",
    "\n",
    "---\n",
    "\n",
    "### 08. VALIDATION WITH DOMAIN KNOWLEDGE\n",
    "\n",
    "---\n",
    "\n",
    "### 09. CATEGORICAL VARIABLES CLEANING\n",
    "\n",
    "---\n",
    "\n",
    "### 10. TEXT DATA CLEANING\n",
    "\n",
    "---\n",
    "\n",
    "### 11. TEMPORAL DATA\n",
    "\n",
    "---\n",
    "\n",
    "### 12. GEOSPATIAL DATA\n",
    "\n",
    "---\n",
    "\n",
    "### 13. NUMERICAL DATA\n",
    "\n",
    "---\n",
    "\n",
    "### 14. FEATURE ENGINEERING (PRE-MODELING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
